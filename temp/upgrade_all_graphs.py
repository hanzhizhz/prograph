#!/usr/bin/env python3
"""
æ‰¹é‡å‡çº§æ‰€æœ‰æ•°æ®é›†çš„å›¾æ–‡ä»¶

ä¿®å¤é—®é¢˜ï¼š
1. SIMILARITY è¾¹æ·»åŠ åå‘è¾¹
2. MENTIONS_ENTITY è¾¹æ·»åŠ åå‘è¾¹
3. direction="1<->2" çš„è¾¹æ·»åŠ åå‘è¾¹
"""

import sys
import pickle
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.proposition_graph.graph_upgrader import GraphUpgrader


# æ•°æ®é›†åˆ—è¡¨
DATASETS = ["HotpotQA", "MuSiQue", "2WikiMultihopQA"]
OUTPUT_DIR = project_root / "output"

# éœ€è¦å‡çº§çš„å›¾æ–‡ä»¶
GRAPH_FILES = [
    "proposition_graph/raw_graph.pkl",
    "proposition_graph/linked_graph.pkl",
    "proposition_graph/raw_graph_fixed.pkl",
]


def upgrade_graph_file(input_path: Path, backup: bool = True, dry_run: bool = False) -> dict:
    """å‡çº§å•ä¸ªå›¾æ–‡ä»¶

    Args:
        input_path: å›¾æ–‡ä»¶è·¯å¾„
        backup: æ˜¯å¦åˆ›å»ºå¤‡ä»½
        dry_run: é¢„è§ˆæ¨¡å¼ï¼Œä¸å®é™…ä¿®æ”¹æ–‡ä»¶

    Returns:
        å‡çº§ç»Ÿè®¡ä¿¡æ¯
    """
    print(f"\n{'='*60}")
    print(f"å¤„ç†: {input_path.relative_to(project_root)}")

    if not input_path.exists():
        print(f"  âš  æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡")
        return None

    # åŠ è½½å›¾
    print(f"  åŠ è½½å›¾...")
    with open(input_path, 'rb') as f:
        graph = pickle.load(f)

    print(f"  åŸå§‹å›¾: {graph.number_of_nodes()} èŠ‚ç‚¹, {graph.number_of_edges()} è¾¹")

    if dry_run:
        print(f"  [é¢„è§ˆæ¨¡å¼] ä¸ä¼šä¿®æ”¹æ–‡ä»¶")

    # å‡çº§
    upgrader = GraphUpgrader(graph)
    upgraded_graph = upgrader.upgrade()

    # ä¿å­˜ï¼ˆéé¢„è§ˆæ¨¡å¼ï¼‰
    if not dry_run:
        # åˆ›å»ºå¤‡ä»½
        if backup:
            backup_path = input_path.with_suffix('.pkl.bak')
            print(f"  åˆ›å»ºå¤‡ä»½: {backup_path.name}")
            with open(input_path, 'rb') as f:
                backup_data = f.read()
            with open(backup_path, 'wb') as f:
                f.write(backup_data)

        # ä¿å­˜å‡çº§åçš„å›¾
        print(f"  ä¿å­˜å‡çº§åçš„å›¾...")
        with open(input_path, 'wb') as f:
            pickle.dump(upgraded_graph, f)

    print(f"  å‡çº§å: {upgraded_graph.number_of_nodes()} èŠ‚ç‚¹, {upgraded_graph.number_of_edges()} è¾¹")
    upgrader.print_stats()

    return upgrader.upgrade_stats


def upgrade_dataset(dataset: str, backup: bool = True, dry_run: bool = False) -> dict:
    """å‡çº§å•ä¸ªæ•°æ®é›†çš„æ‰€æœ‰å›¾æ–‡ä»¶

    Args:
        dataset: æ•°æ®é›†åç§°
        backup: æ˜¯å¦åˆ›å»ºå¤‡ä»½
        dry_run: é¢„è§ˆæ¨¡å¼ï¼Œä¸å®é™…ä¿®æ”¹æ–‡ä»¶

    Returns:
        è¯¥æ•°æ®é›†çš„å‡çº§ç»Ÿè®¡
    """
    print(f"\n\n{'#'*60}")
    print(f"# æ•°æ®é›†: {dataset}")
    if dry_run:
        print(f"# [é¢„è§ˆæ¨¡å¼]")
    print(f"{'#'*60}")

    dataset_dir = OUTPUT_DIR / dataset
    if not dataset_dir.exists():
        print(f"âš  æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨: {dataset_dir}")
        return None

    total_stats = {
        "similarity_edges_added": 0,
        "mention_edges_added": 0,
        "bidirectional_rst_added": 0
    }

    for graph_file in GRAPH_FILES:
        graph_path = dataset_dir / graph_file
        stats = upgrade_graph_file(graph_path, backup=backup, dry_run=dry_run)
        if stats:
            for key in total_stats:
                total_stats[key] += stats[key]

    return total_stats


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="ProGraph å›¾å‡çº§å·¥å…·")
    parser.add_argument("--dry-run", action="store_true", help="é¢„è§ˆæ¨¡å¼ï¼Œä¸å®é™…ä¿®æ”¹æ–‡ä»¶")
    parser.add_argument("--no-backup", action="store_true", help="ä¸åˆ›å»ºå¤‡ä»½æ–‡ä»¶")
    parser.add_argument("--dataset", nargs="+", choices=DATASETS + ["all"], default=["all"],
                        help="è¦å¤„ç†çš„æ•°æ®é›†ï¼Œé»˜è®¤ all")

    args = parser.parse_args()

    print("="*60)
    print("ProGraph å›¾å‡çº§å·¥å…·")
    print("="*60)

    if args.dry_run:
        print("[é¢„è§ˆæ¨¡å¼] ä¸ä¼šä¿®æ”¹ä»»ä½•æ–‡ä»¶")

    # ç¡®å®šè¦å¤„ç†çš„æ•°æ®é›†
    datasets = DATASETS if "all" in args.dataset else args.dataset

    # æ˜¯å¦åˆ›å»ºå¤‡ä»½
    backup = not args.no_backup

    # æ€»è®¡
    grand_total = {
        "similarity_edges_added": 0,
        "mention_edges_added": 0,
        "bidirectional_rst_added": 0
    }

    # å¤„ç†æ¯ä¸ªæ•°æ®é›†
    for dataset in datasets:
        stats = upgrade_dataset(dataset, backup=backup, dry_run=args.dry_run)
        if stats:
            for key in grand_total:
                grand_total[key] += stats[key]

    # æ‰“å°æ€»è®¡
    print("\n\n" + "="*60)
    print("æ€»è®¡")
    print("="*60)
    print(f"SIMILARITY åå‘è¾¹æ·»åŠ : {grand_total['similarity_edges_added']}")
    print(f"MENTIONS_ENTITY åå‘è¾¹æ·»åŠ : {grand_total['mention_edges_added']}")
    print(f"åŒå‘ RST è¾¹æ·»åŠ : {grand_total['bidirectional_rst_added']}")
    print(f"æ€»è®¡æ·»åŠ è¾¹: {sum(grand_total.values())}")

    if args.dry_run:
        print("\n[é¢„è§ˆæ¨¡å¼å®Œæˆ] ä½¿ç”¨ --no-dry-run æ¥å®é™…æ‰§è¡Œå‡çº§")
    else:
        print("\nâœ“ å‡çº§å®Œæˆ!")
        if backup:
            print("ğŸ’¾ å¤‡ä»½æ–‡ä»¶å·²ä¿å­˜ä¸º *.pkl.bak")
            print("   å¦‚éœ€å›æ»šï¼Œå¯ä»¥ä½¿ç”¨å¤‡ä»½æ–‡ä»¶æ¢å¤")


if __name__ == "__main__":
    main()
